{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Demo:  Document Clustering and Latent Semantic Analysis\n",
    "\n",
    "An important application of clustering is for sorting documents into groups.  In this demo, we will illustrate how to use the k-means algorithms for this task.  This example is taken mostly from one of the [sklearn examples](http://scikit-learn.org/stable/auto_examples/text/document_clustering.html).\n",
    "\n",
    "Through the demo, you will learn how to:\n",
    "* Represent a corpus as a set of strings\n",
    "* Build a vocabulary from a corpus\n",
    "* Compute the TF-IDF scores for the documents in the corpus based on the vocabulary\n",
    "* Run k-means to automatically discover document clusters\n",
    "* Display key terms in each document cluster\n",
    "* Perform an LSA on a corpus with a sparse SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "First, we load the standard packages along with a number of `sklearn` sub-packages for text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [UseNet newsgroups](https://en.wikipedia.org/wiki/Usenet_newsgroup) were popular 20 years ago as online forums for discussing various issues.  Although they are not used much today for topic discussions, the posts from that era are still widely-used in machine learning classes for demonstrating various text processing methods.  Due to their wide use, the `sklearn` package has a built-in routine `fetch_20newsgroups` for extracting the newsgroup examples.  We will extract just four of the 20 categories in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "#categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `np.unique` command to compute the number of unique labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset.target\n",
    "true_k = len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in the `data` field of `dataset`.  Each entry `dataset.data[i]` is a string corresponding to the post to the newsgroup.  We can print an example as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post from comp.graphics\n",
      "\n",
      "From: richter@fossi.hab-weimar.de (Axel Richter)\n",
      "Subject: True Color Display in POV\n",
      "Keywords: POV, Raytracing\n",
      "Nntp-Posting-Host: fossi.hab-weimar.de\n",
      "Organization: Hochschule fuer Architektur und Bauwesen Weimar, Germany\n",
      "Lines: 6\n",
      "\n",
      "\n",
      "Hallo POV-Renderers !\n",
      "I've got a BocaX3 Card. Now I try to get POV displaying True Colors\n",
      "while rendering. I've tried most of the options and UNIVESA-Driver\n",
      "but what happens isn't correct.\n",
      "Can anybody help me ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_ind = 10  # Index of an example document\n",
    "data_ex = dataset.data[doc_ind]\n",
    "cat_ex  = dataset.target_names[labels[doc_ind]]\n",
    "print('Post from {0:s}'.format(cat_ex))\n",
    "print()\n",
    "print(data_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing TF-IDF features\n",
    "\n",
    "Documents are natively text.  To apply most machine learning algorithms, we need to convert the documents to vectors.  One popular method is the so-called TF-IDF score.  First, we select a set of words in the corpus.  Each word is sometimes called a *token*.  For each token `n` and document `i`, we then compute the data matrix:\n",
    "      \n",
    "    X[n,i] = TF-IDF score of word i in document n\n",
    "           = term freq[n,i] * inverse doc frequency[i]\n",
    "           \n",
    "where\n",
    "\n",
    "    term freq[n,i]  = (#occurances of word i in doc n)/(#words in doc n)  \n",
    "    inverse doc freq[i] = log(#docs in corpus/#docs with word i)\n",
    "        \n",
    "In the data matrix `X`, each document `n` is represented by a vector `X[n,:]`.\n",
    "\n",
    "The data matrix `X` can be computed by a *vectorizer*.  Writing an efficient vectorizer is somewhat time-consuming.  Luckily, `sklearn` has very good routines to compute the TF-IDF representations of a corpus.  We first create a `TfidfVectorizer` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer( \n",
    "                max_df=0.5, # max doc freq (as a fraction) of any word to include in the vocabulary\n",
    "                min_df=2,   # min doc freq (as doc counts) of any word to include in the vocabulary\n",
    "                max_features=10000,           # max number of words in the vocabulary\n",
    "                stop_words='english',         # remove English stopwords\n",
    "                use_idf=True )        # use IDF scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create the data matrix from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training dataset using a sparse vectorizer\n",
      "done in 1.005443s\n",
      "n_samples: 3387, n_features: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the terms with the highest TF-IDF scores in a post as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weimar               0.565396 \n",
      "pov                  0.518174 \n",
      "renderers            0.183033 \n",
      "univesa              0.178595 \n",
      "und                  0.174842 \n",
      "fuer                 0.171591 \n",
      "true                 0.159214 \n",
      "raytracing           0.150534 \n",
      "displaying           0.140240 \n",
      "ve                   0.139752 \n",
      "options              0.134309 \n",
      "rendering            0.133027 \n",
      "driver               0.129544 \n",
      "happens              0.122540 \n",
      "colors               0.119138 \n",
      "card                 0.113776 \n",
      "display              0.108457 \n",
      "germany              0.108231 \n",
      "tried                0.106282 \n",
      "color                0.103717 \n",
      "anybody              0.100397 \n",
      "correct              0.100234 \n",
      "isn                  0.084694 \n",
      "got                  0.081865 \n",
      "keywords             0.081865 \n",
      "try                  0.080601 \n",
      "help                 0.078058 \n",
      "nntp                 0.044277 \n",
      "host                 0.043985 \n",
      "posting              0.042608 \n"
     ]
    }
   ],
   "source": [
    "doc_ind = 10  # Index of an example document\n",
    "xi = X[doc_ind,:].todense()\n",
    "term_ind = xi.argsort()[:, ::-1]\n",
    "xi_sort = xi[0,term_ind]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(30):\n",
    "    term = terms[term_ind[0,i]]\n",
    "    tfidf = xi[0,term_ind[0,i]]\n",
    "    print('{0:20s} {1:f} '.format(term, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run k-Means\n",
    "\n",
    "We now run k-means on the TF-IDF vectors to try  to automatically detect clusters.  First, we construct a `kMeans` object to perform the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Then, we run the k-means clustering.  This will run through several iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
      "    n_clusters=4, n_init=1, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=None, tol=0.0001, verbose=True)\n",
      "Initialization complete\n",
      "Iteration  0, inertia 6466.354\n",
      "Iteration  1, inertia 3296.793\n",
      "Iteration  2, inertia 3280.722\n",
      "Iteration  3, inertia 3277.321\n",
      "Iteration  4, inertia 3275.984\n",
      "Iteration  5, inertia 3275.012\n",
      "Iteration  6, inertia 3274.164\n",
      "Iteration  7, inertia 3273.590\n",
      "Iteration  8, inertia 3273.193\n",
      "Iteration  9, inertia 3272.871\n",
      "Iteration 10, inertia 3272.423\n",
      "Iteration 11, inertia 3271.898\n",
      "Iteration 12, inertia 3271.128\n",
      "Iteration 13, inertia 3269.584\n",
      "Iteration 14, inertia 3268.490\n",
      "Iteration 15, inertia 3268.193\n",
      "Iteration 16, inertia 3267.900\n",
      "Iteration 17, inertia 3267.508\n",
      "Iteration 18, inertia 3267.340\n",
      "Iteration 19, inertia 3267.208\n",
      "Iteration 20, inertia 3266.912\n",
      "Iteration 21, inertia 3266.866\n",
      "Iteration 22, inertia 3266.837\n",
      "Iteration 23, inertia 3266.802\n",
      "Iteration 24, inertia 3266.693\n",
      "Iteration 25, inertia 3266.642\n",
      "Iteration 26, inertia 3266.635\n",
      "Iteration 27, inertia 3266.631\n",
      "Iteration 28, inertia 3266.627\n",
      "Iteration 29, inertia 3266.626\n",
      "Converged at iteration 29: center shift 0.000000e+00 within tolerance 9.816505e-09\n",
      "done in 11.236s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the clusters, we print out the terms corresponding to the 10 largest components of the centroid in each cluster.  You can clearly see the clustering of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: space nasa henry access digex toronto alaska pat gov shuttle\n",
      "Cluster 1: god com people don say jesus article think bible christian\n",
      "Cluster 2: graphics com university image posting thanks host nntp computer ac\n",
      "Cluster 3: sandvik sgi livesey kent com apple newton solntze wpd jon\n"
     ]
    }
   ],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of clusters to newsgroup categories\n",
    "\n",
    "The clusters found by k-means were not based on the newsgroup category in which the post came from.  To compare the two, we create a sort of confusion matrix where:\n",
    "\n",
    "`C[i,j] = ` fraction of cluster `j` came from newsgroup `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.57470211 0.03006012 0.60189573]\n",
      " [0.01020408 0.00458295 0.64261857 0.        ]\n",
      " [0.9829932  0.01099908 0.26519706 0.        ]\n",
      " [0.00680272 0.40971586 0.06212425 0.39810427]]\n"
     ]
    }
   ],
   "source": [
    "labelkm = km.labels_\n",
    "from sklearn.metrics import confusion_matrix\n",
    "C = confusion_matrix(labels,labelkm)\n",
    "\n",
    "Csum = np.sum(C,axis=0)\n",
    "Cnorm = C / Csum[None,:]\n",
    "print(Cnorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret this confusion matrix, let's print out the newsgroup names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, returning to the confusion matrix, we see that some discovered clusters lie almost entirely within one of the newsgroup subjects.  This is especially true for `comp.graphics` and `sci.space`.  However, some discovered clusters tend to have entries of both `alt.atheism` and `talk.religon.misc`, whose topics are likely to have a lot of overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print an example of a post that came from a newsgroup that is different from the most common newsgroup in that cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual newsgroup: talk.religion.misc\n",
      "Most common newsgroup in cluster:  alt.atheism\n",
      "\n",
      "From: skinner@sp94.csrd.uiuc.edu (Gregg Skinner)\n",
      "Subject: Re: Davidians and compassion\n",
      "Reply-To: g-skinner@uiuc.edu\n",
      "Organization: UIUC Center for Supercomputing Research and Development\n",
      "Lines: 26\n",
      "\n",
      "sandvik@newton.apple.com (Kent Sandvik) writes:\n",
      "\n",
      ">In article <1993Apr20.143400.569@ra.royalroads.ca>, mlee@post.RoyalRoads.ca\n",
      ">(Malcolm Lee) wrote:\n",
      ">> Do you judge all Christians by the acts of those who would call\n",
      ">> themselves Christian and yet are not?  The BD's contradicted scripture\n",
      ">> in their actions.  They were NOT Christian.  Simple as that.  Perhaps\n",
      ">> you have read too much into what the media has portrayed.  Ask any\n",
      ">> true-believing Christian and you will find that they will deny any\n",
      ">> association with the BD's.  Even the 7th Day Adventists have denied any\n",
      ">> further ties with this cult, which was what they were.\n",
      "\n",
      ">Well, if they were Satanists, or followers of an obscure religion,\n",
      ">then I would be sure that Christians would in unison condemn and \n",
      ">make this to a show case.\n",
      "\n",
      "You might be sure, but you would also be wrong.\n",
      "\n",
      ">And does not this show the dangers with religion -- in order \n",
      ">word a mind virus that will make mothers capable of letting\n",
      ">their small children burn to ashes while they scream?\n",
      "\n",
      "I suspect the answer to this question is the same as the answer to,\n",
      "\"Do not the actions of the likes of Stalin show the dangers of\n",
      "atheism?\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "I = np.where((labels==3) & (labelkm == 3))[0]\n",
    "doc_ind = I[3]\n",
    "ind_cluster = labelkm[doc_ind]\n",
    "km_cat = dataset.target_names[np.argmax(Cnorm[:,ind_cluster])]\n",
    "\n",
    "data_ex = dataset.data[doc_ind]\n",
    "true_cat  = dataset.target_names[labels[doc_ind]]\n",
    "print('Actual newsgroup: {0:s}'.format(true_cat))\n",
    "print('Most common newsgroup in cluster:  {0:s}'.format(km_cat))\n",
    "print()\n",
    "print(data_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Latent Semantic Analysis\n",
    "\n",
    "Another important tool in document analysis is [latent semantic analysis (LSA)](https://en.wikipedia.org/wiki/Latent_semantic_analysis).  In LSA, we simply compute an SVD of the TF-IDF matrix,\n",
    "\n",
    "    X = U diag(S) V\n",
    "    \n",
    "This is equivalent to performing a PCA on `X`.  If we let `A = U diag(S)` then `X = AV`.  First, we compute the PCs of `X`.  Since `X` is a sparse matrix, it is preferable to use the sparse `svds` method in the `scipy` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse.linalg\n",
    "U1,S1,V1 = scipy.sparse.linalg.svds(X,k=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can next plot the singular values. We see that the first few singular values are significantly larger than the remaining singular values suggesting that the term-document matrix `X` has a low rank structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a2781b6a0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(S1[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the low rank structure of `X`, we can write \n",
    "\n",
    "     X[n,i] = \\sum_k A[n,k] V[k,i]\n",
    "     \n",
    "where the sum is over some relatively small number of components.  There are two uses for this representation:\n",
    "* Word and document embeddings:  A[n,:] provides a low-dimensional vector representation of each document.  This is useful pre-processing step in many natural processing (NLP) methods.  This type of representation is closely related to an important topic of *word embeddings* and *document embeddings*.\n",
    "* Topic modeling:  One interpretation of the PCA is that each PC `k` represents some common *topic* in the corpus.  Then, `A[n,k] =` the component of topic `k` in document `n` and `V[k,i]` represents the occurance of word `i` in topic `k`.\n",
    "\n",
    "To get an idea of the words within each PC, we print the words for the largest components in the first 5 PCs.  On a small corpus like 20 newsgroups, the PCs in this case are not very useful.  But, the technique can yield more useful results in larger corpi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC 0: muenchen does dan bockamp targa colour shall ch uk man\n",
      "PC 1: phigs government uci 3d khomeini colorado uni rh rayshade screen\n",
      "PC 2: clarke asimov wesleyan values vga physics fl ed pluto tyre\n",
      "PC 3: ericsson color wesleyan point program convenient boeing scott targa jpeg\n",
      "PC 4: thanks muenchen format mac earth pluto color uci true ether\n"
     ]
    }
   ],
   "source": [
    "V1sort = np.abs(V1).argsort()[:, ::-1]\n",
    "for i in range(5):\n",
    "    print(\"PC %d:\" % i, end='')\n",
    "    for ind in V1sort[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
